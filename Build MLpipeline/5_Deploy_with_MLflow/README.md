
# Fetch the production model
First we need to fetch the production model. We are going to save it into the ``model``
directory:

--root model: means setting up the saving root directory of the artifact downloaded as "model"

```bash
wandb artifact get genre_classification_prod/model_export:prod --root model
```

# Offline (batch) inference
Let's run inference on the test set. 

## Steps
1. Use the W&B CLI to download the ``genre_classification_prod/data_test.csv:latest`` artifact
   locally:

```bash
wandb artifact get genre_classification_prod/data_test.csv:latest
```

2. Use ``mlflow models predict ...`` to perform batch inference on it:
-t: input file format 
-i: input file
-m: directory that contains the model/artifact
--env-manager=conda: Specifies that Conda will be used to set up the environment for running the model (use conda.yaml file in the model's directory) to ensure all dependencies are installed and compatible.

```bash
mlflow models predict -t csv -i ./artifacts/data_test.csv:v0/data_test.csv -m model --env-manager=conda
```
   
# Online inference
Here we use the model for online prediction exposing the model as a REST API.

1. Create a REST API to perform online inference (by default, the endpoint of API will be localhost)

```bash
mlflow models serve -m model --env-manager=conda &
```
   check endpoint shown in the output: Listen at: http://127.0.0.1:5000 <==http://localhost:5000>
```bash
kill <end point number>
```
2. Open Jupyter in another terminal:
   ```bash
   jupyter notebook
   ```

   In the notebook, use the ``requests`` library to interrogate the API and do inference on the provided ``data_sample.json``:
   ```python
   import requests
   import json

   #check the files inside "model" directory
   !ls model

   with open("data_sample.json") as fp:
       data = json.load(fp)
   #add "/invocations" to interrogate with the API generated by MLFlow 
   results = requests.post("http://localhost:5000/invocations", json=data)
   
   print(results.json())
   ```
   
## Bonus: docker deployment
You can also use docker to build an image and then deploy to a Cloud provider 
(AWS, GCP, Azure...). Expose port 5000 of that machine to the world, and you will be able to
use your model from whenever as a simple API call.
1. Ensure your user is part of the docker group by running the following:
```bash
   groups
```
Look for docker in the output. If it's missing, you'll need to add your user to the docker group:
   1. sudo usermod -aG docker $USER
   2. newgrp docker
2. Create the docker image for serving the model as API:
   ```bash
   mlflow models build-docker -m model -n "genre_classification"
   ```
   -n flag specifies the name of the Docker image
   -m flag specifies the path or URI to the MLflow model directory 

   This will take a few minutes (of course, you need docker installed)
3. Tag the image
Before pushing the image to dockerhub, you need to tag it with your Docker Hub username and the repository name:

```bash
   docker tag <local_image> <dockerhub_username>/<repository_name>:<tag>
```
Here is:
<tagged as 1.0>
```bash
   docker tag genre_classification zhiconghuang/genre_classification_demo:1.0
```
or <tagged as latest>
```bash
   docker tag genre_classification zhiconghuang/genre_classification_demo
```
This doesn't create a new image; it simply adds a new tag to the existing image.<tagging an imgae for versioning>

4. Push a new tag/Tagged Image to a repository(ex:genre_classification_demo) in dockerhub
```bash
docker push <dockerhub_username>/<repository_name>:<tag>
```
Here is:
```bash
docker push zhiconghuang/genre_classification_demo:1.0
```
4. Follow the procedure for your Cloud provider of choice to deploy a Docker image

4. Open port 5000 on the machine hosting the image

5. Use requests to interrogate that machine, by using the snippet we used earlier and substituting
   ``http://localhost:5000/invocations`` with ``[url to the deployed machine]:5000/invocations``